import streamlit as st
import ollama
from openai import OpenAI
from ollama import Client
import numpy as np
import pandas as pd
# from langchain_community.chat_models import ChatOllama

from litellm import completion

st.title("KPI Assistant")


# handles stream response back from LLM
def stream_parser(stream):
    for chunk in stream:
        yield chunk['message']['content']
        
def stream_parser_litellm(stream):
    for chunk in stream:
        if chunk['choices'][0]['delta']['content']:  # Check for non-None content
            yield chunk['choices'][0]['delta']['content']
        else:
            print(f"Skipping None completion: {chunk}")  # Optionally log skipped completions

    # return stream['choices'][0]['message']['content']

def generate_chart(response):
    """
    Generates a chart (line or bar) based on the completion text.

    Args:
        completion: The text generated by the completion function.

    Returns:
        A pandas DataFrame suitable for plotting with Streamlit, or None if no chart is applicable.
    """

    # Chart type determination (can be extended for more chart types)
    if "trend" in response.lower():
        chart_type = "line"
    elif "comparison" in response.lower():
        chart_type = "bar"
    else:
        chart_type = None

    # Data extraction (replace with your logic based on completion format)
    # This is a placeholder, assuming some data extraction from completion
    # data = {"Year": [2020, 2021, 2022, 2023], "Value": [10, 15, 20, 25]}
    # df_plot = pd.DataFrame(data)

    if chart_type:
        return df_plot  # Return DataFrame for plotting if chart type is determined
    else:
        return None  # Return None if no chart is applicable

client = Client(host="http://localhost:11434")

df_data = pd.read_csv("./data/energy_production.csv")
df_plot = pd.melt(df_data,
        id_vars = ['Year'],
        value_vars = ['Population(M)', 'Coal', 'Oil', 'Gas', 'Nuclear'],
        var_name = 'Metric',
        value_name = 'Value'
)

if "model" not in st.session_state:
    # st.session_state["model"] = "gpt-3.5-turbo"
    st.session_state["model"] = "mistral"


if "messages" not in st.session_state:
    st.session_state.messages = []

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("What is up?"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        # response = st.write_stream(stream)
        # stream = ChatOllama(base_url="http://localhost:11434", model="mistral", streaming=True)
        # stream = ChatOllama(
        # stream = client.chat(
        #     model=st.session_state["model"],
        #     # model="mistral",
        #     # model='llama3',
        #     # messages=[
        #     #     {'role': 'user', 'content': 'Why is the sky blue?'}
        #     #     ],
        #     messages=[
        #         {"role": m["role"], "content": m["content"]}
        #         for m in st.session_state.messages
        #     ],
        #     # messages=[
        #     #     {"role": "user", "content": prompt}              
        #     # ],            
        #     stream=True,
        # )

        stream = completion(
            model="ollama/llama2",
            # model="mistral",
            # model='llama3',
            # messages=[
            #     {'role': 'user', 'content': 'Why is the sky blue?'}
            #     ],
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            # messages=[
            #     {"role": "user", "content": prompt}              
            # ],            
            api_base="http://localhost:11434",
            stream=True,
        )
        # response = st.write_stream(stream_parser(stream))
        response = st.write_stream(stream_parser_litellm(stream))

        # st.line_chart(df_plot, x="Year", y="Value", color="Metric")
        generate_chart(response)
        

        
    st.session_state.messages.append({"role": "assistant", "content": response})